# Да что такое этот ваш containerd? Как работает контейнеризация? Связь с Docker и Kubernetes

![what_is_containerd](https://github.com/AnatoliyBr/2022_2023-introduction_to_distributed_technologies-k4111c-briushinin_a_a/blob/master/containerd_medium_paper/images/what_is_containerd.png 'What is containerd?')

## Введение
Данная статья будет полезна тем, кто уже знаком с технологиями Docker и Kubernetes, но хочет разобраться, как работает среда выполнения контейнеров, в частности containerd, и как она связана с такими понятиями, как runC, OCI, CRI и CNI.

## Содержание
* Это база. Процессы ОС
* Функции ядра Linux и контейнеры
* Среды управления контейнерами и стандартизация OCI
* Архитектура containerd
* Связь с Docker и Kubernetes
* Заключение
* Полезные ссылки

## Это база

### Процесс – основное понятие ОС
Чтобы разобраться, как работают контейнеры, надо вспомнить про [**операционную систему** (ОС)](https://ru.wikipedia.org/wiki/Операционная_система) - связующее звено между физическими компонентами компьютера и пользователем. Ключевым элементом ОС является [**ядро** (kernel)](https://ru.wikipedia.org/wiki/Ядро_операционной_системы), состоящее из множества **объектов**.

Одним из них является [**процесс**](https://sysadminium.ru/processes-in-a-linux-system/) - среда выполнения для работы экземпляра программы (в ядре это выглядит, как [структура](https://habr.com/ru/post/423049/) с множеством полей). Процесс является основой функционирования [UNIX-подобных ОС](https://ru.wikipedia.org/wiki/Unix-подобная_операционная_система), таких как Linux.

![os_process_diagram](https://github.com/AnatoliyBr/2022_2023-introduction_to_distributed_technologies-k4111c-briushinin_a_a/blob/master/containerd_medium_paper/images/os_process_diagram.png 'os process diagram')

UNIX является многозадачной ОС, то есть позволяет одновременно запустить несколько программ. Когда пользователь запускает программу, для нее создается процесс, в рамках которого и работает программа. У каждого процесса есть **уникальный идентификатор (process ID, PID)**, некоторые свойства, также ему выделяется часть оперативной памяти.

### Системные вызовы fork() и exec()
Процессы порождаются **системными вызовами (syscalls)** - специальными функциями, обеспечивающими взаимодействие системного ПО с ядром ОС. У каждого процесса есть **родительский**, за исключением init-процесса (PID = 1).

Процессы с PID равным 0 и 1 запускаются ядром **автоматически** во время запуска системы. Прочитать про формирование процессов 0 и 1 можно [здесь](https://cyberpedia.su/12x1254f.html).

Для создания нового процесса используют связку из двух системных вызовов: `fork()` и `exec()`.

Первоначально с помощью системного вызова `fork()` создают **копию** существующего процесса (изначально, это init-процесс). Оба процесса абсолютно идентичны: у них одинаковое значение локальных переменных, состояние памяти и т.д. 

> Чтобы не копировать каждый раз память родительского процесса, используется технология [Copy-On-Write](https://ru.wikipedia.org/wiki/Копирование_при_записи).

В случае успеха родительскому процессу возвращается PID нового процесса, а дочернему 0.

Затем с помощью системного вызова `exec()` **заменяют** дочерний процесс на другой - загружают в пространство процесса **новую программу**. При этом PID сохраняется.

### Завершение процесса
Для завершения процесса используют сигналы `sigterm` ("просьба" завершиться - процесс может его проигнорировать) и `sigkill` ("приказ" - его может проигнорировать только init-процесс).

Когда дочерний процесс завершается, родитель получает сигнал `sigchld`. Пока этот сигнал не будет обработан, запись об умершем дочернем процессе не может быть удалена из таблицы процессов. Если процесс не существует, но все еще занимает запись в таблице процессов, он называется **zombie-процессом**.

В случае завершения родительского процесса все "сироты" будут **усыновлены** init-процессом (PID = 1).

### В чем идея?
Долгое время системный вызов `fork()` был единственным способом создания процесса, при этом все процессы находились **в общем контексте**: у них была общая память, общее дерево процессов, общая сеть, общие точки монтирования (mounts).

Представьте теперь, что вы запускаете на сервере два приложения. Для каждого из них будет создан процесс, у них будет **общая файловая система**. Это значит, что файлы одного процесса могут **конфликтовать** с файлами другого процесса. В итоге, одно приложение может **влиять** на другое.

Чтобы решить данную проблему, необходимо **изолировать процессы** приложения от остальной системы, кроме того, нужны механизмы **разделения и ограничения ресурсов**.

> Идея: изоляция процессов и управление ресурсами.

## Функции ядра Linux и контейнеры
В основе технологии **контейнеризации** лежат **функции ядра Linux**, а именно
* Namespaces
* Cgroups

**Контейнеры** по сути являются **абстракцией над различными функциями ядра Linux**, с помощью которых реализуется изоляция процессов и контроль ресурсов. Также существует альтернативный метод - использование **виртуальных машин**, но об этом чуть позже.

### chroot
Первой попыткой изолировать процессы можно назвать системный вызов **chroot (change root)**. Он отвечает за **изоляцию** процесса **на уровне файловой системы**, с помощью него можно изменить корневой каталог (вершину иерархии, он же `/` или `root`). Программе, запущенной с изменённым корневым каталогом, будут доступны только файлы, находящиеся в этом каталоге. В итоге, файловая система разделена на части, и они никак не влияют друг на друга.

Недостатки
* Все необходимые файлы нужно дублировать
* Проблемы с безопасностью

### Namespaces
[Пространства имен (namespaces)](https://en.wikipedia.org/wiki/Linux_namespaces) - функция ядра Linux, которая разделяет ресурсы ядра. Она определяет, какие ресурсы ядра процесс "видит", а какие - нет. По сути - это эволюция chroot.

Существует **7 типов** пространства имен: `PID`, `Network`, `User`, `Mount`, `IPC`, `UTS`, `Cgroups`. Каждый тип отвечает за **изоляцию** определенного системного ресурса. PID namespaces изолирует ID процессов, Mount namespaces - файлы и каталоги в системе и так далее. Подробный разбор всех типов namespaces [здесь](https://habr.com/ru/company/selectel/blog/279281/). Также рекомендую прочитать данную [статью](https://habr.com/ru/post/458462/).

Можно представить, что пространство имен - это **ящик**. Допустим, во всей системе есть два системных интерфейса: `wlan0` и `eth0`. Тогда можно создать два пространства имен Network A и B, так что процесс 1, принадлежащий пространству имен A, будет **"видеть"** только карту WiFi (`wlan0`), а процесс 2, принадлежащий пространству имен B, - только карту Ethernet (`eth0`). Таким образом, сетевые ресурсы системы разделились между двумя namespaces'ами.

> Namespaces обеспечивают изоляцию процессов.

Когда появились механизмы пространства имен, на замену системного вызова `fork()` пришел `clone()`, который позволил создать копию процесса **с уникальным пространством имен**.

### Cgroups
[Контрольные группы (cgroups)](https://ru.wikipedia.org/wiki/Контрольная_группа_(Linux)) - функция ядра Linux, которая ограничивает и изолирует использование ресурсов (ЦП, память, дисковый ввод-вывод, сеть и т.д.) для набора процессов.

Механизм cgroups состоит из **2 частей**: **ядра (cgroup core)** и **подсистем**. Каждая подсистема - это директория с **управляющими файлами**, в которых прописываются все настройки. Также у каждой подсистемы есть **собственные управляющие файлы**. Чтобы создать контрольную группу, необходимо создать **вложенную директорию** в любой из подсистем. В эту вложенную директорию будут автоматически добавлены управляющие файлы. Для добавления процесса в контрольную группу нужно записать его PID в управляющий файл `tasks`. Подробный разбор всех подсистем [здесь](https://habr.com/ru/company/selectel/blog/303190/).

Возможности механизма контрольных групп:
* **Ограничение ресурсов** - можно определить сколько CPU или памяти будет использовать конкретный процесс.
* **Приоритизация** - cgroups позволяют определить, какой процесс в рамках одной cgroup'ы более приоритетен для использования ресурсов.
* **Отчетность** - удобные инструменты мониторинга: пределы (limits) ресурсов отслеживаются и сообщаются на уровне cgroup'ы.
* **Управление** - можно изменить статус (frozen, stopped, restarted) всех процессов cgroup'ы одной командой.

> Сgroups обеспечивают ограничение физических мощностей процесса.

### Виртуальные машины vs Контейнеры
До появления контейнеров [виртуальные машины (ВМ)](https://habr.com/ru/company/timeweb/blog/665786/) были **единственным** способом изоляции процессов.

Для запуска ВМ необходим **гипервизор (hypervisor)**, он "нарезает" физическую машину на несколько виртуальных, то есть выделяет каждой ВМ часть CPU, памяти, дискового пространства и т.д.

Гипервизор бывает 2 типов:
1. **Аппаратный** (native, aka bare metal) - устанавливается непосредственно на физическое оборудование
    * KVM, Hyper-V, Xen, VMware ESX
2. **Программный** (hosted) - устанавливается в ОС
    * VirtualBox, VMware, EMU, Parallels

![hypervisors](https://github.com/AnatoliyBr/2022_2023-introduction_to_distributed_technologies-k4111c-briushinin_a_a/blob/master/containerd_medium_paper/images/hypervisors.png 'hypervisors')

Гипервизор управляет ВМ, на которых запускаются **гостевые ОС**.

Таким образом, можно запустить на одном физическом сервере несколько ВМ, на каждой ВМ запустить приложения с разными версиями библиотек, и приложения на разных ВМ не будут влиять друг на друга.

Заметим, что в данном случае происходит изоляция на уровне "железа" и четко прослеживается вложенность (ВМ -> ОС -> приложение).

Недостатки ВМ:
* Долгий запуск (работа гипервизора)
* Большие требования к CPU и RAM
* Занимает десятки Гб диска

Даже если мы запускаем маленькое приложение, требуется много ресурсов на запуск ОС.

Сравним ВМ с контейнеризацией, так называемой **виртуализацией на уровне ОС**.

> Контейнер - это изолированный процесс.

В данном случае приложения запускаются нативно, на **host'овой (базовой) ОС**. Контейнеризация осуществляется с помощью платформ контейнеризации (например, Docker), которые помогают создать **изолированное окружение для запуска приложения**. В контейнерах также как и в ВМ, запускаются один или несколько процессов.

В случае контейнеров **нет** дополнительных расходов на запуск **гостевой ОС**.

**Преимуществами контейнеров** перед ВМ будут:
* Быстрый запуск
* Высокая скорость работы
* Очень легковесны

**Недостатком контейнеров** является то, что запустить контейнер можно только в том случае, если ядро ОС совпадает с тем, для которого написан контейнер. Большинство образов (из которых создаются контейнеры) собраны под Linux, поэтому их нельзя напрямую запустить на другой ОС (Windows, MacOS, FreeBSD, Solaris) - необходимо использовать ВМ Linux.

## Среды управления контейнерами и стандартизация OCI
История развития технологии контейнеризации достаточно богата и подробно описана [здесь](https://habr.com/ru/post/541288/). Рассмотрим основные компоненты экосистемы контейнеров.

### Стандарты OCI, CRI, CNI
**Open Container Initiative (OCI)** - проект, целью которого является унификация стандартов управления контейнерными технологиями. Он содержит две спецификации:
* runtime-spec: спецификация среды выполнения контейнеров
* image-spec: спецификация образа

> **Среды выполнения контейнеров (Container Runtime)** - это программные компоненты, которые могут запускать контейнеры в ОС host'а.

**Container Runtime Interface (CRI)** определяет API между Kubernetes и Container Runtime.
Данный стандарт позволил Kubernetes **не интегрироваться с Docker**, а использовать среды выполнения **напрямую**, если они соответствуют спецификации.

**Container Networking Interface (CNI)**- **набор требований** к исполняемой среде контейнеров (container runtimes) и плагинам (plugins), соответствие которым позволяет любому плагину работать с любым runtime'ом.

CNI состоит из 4 частей:
* Спецификация
* **Плагины CNI (CNI plugin)** - это исполняемые файлы, соответствующие спецификации. Существуют плагины, отвечающие за различные функции **при настройке сети** Pod'а.
* Библиотеки
* Сторонние плагины

Все части подробно описаны [здесь](https://habr.com/ru/company/flant/blog/329830/).

### libcontainer, runC, containerd и Docker
**libcontainer**
* [Библиотека](https://pkg.go.dev/github.com/opencontainers/runc/libcontainer#section-readme) для взаимодействия с **функциями ядра Linux**, такими как namespaces и cgroups, написана на языке Go компанией Docker
* Позволяет управлять **жизненным циклом контейнера**, а также выполнять дополнительные операции после его создания

**runC**
* Легкая портативная **низкоуровневая** среда выполнения контейнеров (low-level container runtime)
* Утилита, **использующая libcontainer**, также написана на языке Go
* Инструмент командной строки для **создания и запуска контейнеров** в соответствии со **спецификацией OCI**
    * запускает контейнерный процесс (системные вызовы `fork()` и `exec()`), а затем завершается
    * подготавливает среду выполнения для контейнера (создает namespaces и cgroups)
* Особенности runС
    * Встроенная поддержка функций безопасности Linux, таких как AppArmor, SELinux и т.д.
    * Встроенная поддержка контейнеров Windows 10

**containerd**
* **Высокоуровневая** среда выполнения контейнеров (high-level container runtime), использует **runC** для запуска контейнеров, но реализует также другие высокоуровневые функции, такие как управление образами и высокоуровневые API
* [Демон](https://thecode.media/daemon/) на host-системе, то есть компьютерная программа, которая выполняется как фоновый процесс, не находится под непосредственным управлением пользователя.
* containerd взаимодействует с runC **не напрямую**
    * сначала **containerd** создает процесс **containerd-shim**, который затем использует **runC** для запуска контейнера (точнее передает ему **bundle (комплект)** образа контейнера)
    * containerd-shim позволяет низкоуровневой среде выполнения (в данном случае runC) **завершиться** после запуска контейнера

## Архитектура containerd
### Зачем нужен containerd-shim?
* containerd-shim является **родительским процессом** для контейнерного процесса
* следит за **статусом** контейнерного процесса и информирует containerd
* В случае завершения контейнерного процесса с PID равным 1 containerd-shim **убивает дочерние процессы** контейнерного процесса, чтобы не копились **zombie-процессы**
* Если родительским процессом был бы containerd, то в случае зависания containerd, все контейнеры на всем host'е должны были бы завершиться - containerd-shim позволяет решить эту проблему

> Необходимо убивать zombie-процессы, иначе рано или поздно **закончится** допустимое число PID в системе.

### Что такое bundles?
**bundles («комплекты») и контейнеры** - основные примитивы, с которыми работает containerd.

Bundles содержат конфигурацию, метаданные и данные корневой файловой системы. Они являются дисковым представлением запущенного контейнера (в простейшем случае - это обычный каталог в файловой системы), которое можно переносить на другие системы, упаковывать и распространять.

> По сути всё устройство containerd заключается в том, чтобы координировать создание и запуск bundles.

### Подсистемы containerd
Компоненты containerd образуют следующие 3 подсистемы:
* **Distribution** - сервис, обеспечивающий получение образов контейнеров.
* **Bundle** - сервис, позволяющий извлекать (и упаковывать) bundles из дисковых образов.
* **Runtime** - сервис для запуска bundles (вызывает runC для запуска контейнеров с переданными им параметрами).

Каждая подсистема имеет один или несколько **компонентов**, реализующих её поведение. Именно к сервисам, предоставляемым подсистемами, и обращаются пользователи containerd через [gRPC API](https://ru.wikipedia.org/wiki/GRPC).

Компоненты containerd, работающие одновременно с разными подсистемами, называются **модулями**. Работа всех модулей подробно разобрана в [официальном репозитории](https://github.com/docker-archive/containerd/blob/master/design/architecture.md) и переведена [здесь](https://habr.com/ru/company/flant/blog/325358/).

## Связь с Docker и Kubernetes
**containerd** - это высокоуровневая **среда выполнения контейнеров**, которую Docker и Kubernetes используют для управления жизненным циклом контейнеров.

### Связь с Docker
Следующая схема наглядно показывает место containerd в Docker.

![docker_containerd_diagram](https://github.com/AnatoliyBr/2022_2023-introduction_to_distributed_technologies-k4111c-briushinin_a_a/blob/master/containerd_medium_paper/images/docker_containerd_diagram.jpg 'docker_containerd_diagram')

### Связь с Kubernetes
**Kubernetes** - платформа для оркестрации контейнеров. Она может использовать в качестве среды выполнения контейнеров все, что удовлетворяет стандарту OCI через CRI.

Кроме этого для **настройки сети** Kubernetes использует CNI-plugin'ы, это сделано все по той же причине что и введение CRI - чтобы не зависеть от конкретного сетевого провайдера.

Важным архитектурным принципом сетевого устройства Kubernetes является то, что **каждый Pod обладает уникальным IP**. IP пода делится между всеми его контейнерами и является доступным (маршрутизируемым) для всех остальных Pod'ов.

**Что такое pause-контейнеры?**  
Их ещё называют «контейнерами-песочницами» (sandbox containers) - резервируют и удерживают сетевое пространство имён (Network namespace), используемого всеми контейнерами Pod'а. Благодаря этому IP пода не меняется даже в тех случаях, когда контейнер умирает и вместо него создаётся новый.

Большим достоинством такой модели — IP для каждого пода (IP-per-pod) — является отсутствие коллизий IP/портов на нижележащем host'е. А разработчик может не беспокоиться, какие порты используют приложения.

Когда Pod планируется на узел (node), kubelet вызывает CRI-плагин для его создания. Далее, если используется containerd, плагин Containerd CRI вызывает CNI-плагин, указанный в конфиге CNI, для настройки сети Pod'а. В результате Pod получает IP-адрес.

Взаимодействие Kubernetes с containerd через CRI наглядно представлено на следующей схеме:

![k8s_cri](https://github.com/AnatoliyBr/2022_2023-introduction_to_distributed_technologies-k4111c-briushinin_a_a/blob/master/containerd_medium_paper/images/k8s_cri.png 'k8s_cri')

Рассмотрим подробнее, как работает CRI-plugin для случая, когда kubelet создает Pod с одним контейнером:

![containerd_k8s_cri_architecture](https://github.com/AnatoliyBr/2022_2023-introduction_to_distributed_technologies-k4111c-briushinin_a_a/blob/master/containerd_medium_paper/images/containerd_k8s_cri_architecture.png 'containerd_k8s_cri_architecture')

* kubelet вызывает CRI-plugin через API CRI runtime service для создания Pod'аю.
* CRI использует containerd для создания и запуска pause-контейнера (sandbox-контейнера) и размещения этого контейнера внутри cgroups и namespaces Pod'а.
* CRI настраивает сетевое пространство имен (Network namespace) Pod'а с помощью CNI.
* Далее kubelet вызывает CRI-plugin через API CRI image service, чтобы извлечь образ контейнера приложения
* CRI далее использует containerd для извлечения образа, если образа нет на узле (Node).
* Затем kubelet вызывает CRI через CRI runtime service API, чтобы создать и запустить контейнер приложения внутри Pod'а, используя извлеченный образ контейнера.
* В итоге CRI использует containerd для создания контейнера приложения, добавляет его в контрольную группу и пространство имен Pod'а, а затем для запуска нового контейнера приложения Pod'а. После этих шагов создается и запускается Pod и соответствующий ему контейнер приложения.

## Заключение
Итак, мы выяснили, что:
* Контейнеры позволяют запустить приложения в изолированном окружении
* В основе контейнеров лежат функции ядра Linux
* containerd используется платформами контейнеризации и оркестрации приложений в качестве среды выполнения контейнеров
* Существует множество стандартов, которые позволяют не зависеть от конкретного производителя

## Полезные ссылки
* [Официальный репозиторий containerd](https://github.com/containerd/containerd)
* [Механизмы контейнеризации: namespaces](https://habr.com/ru/company/selectel/blog/279281/)
* [Механизмы контейнеризации: cgroups](https://habr.com/ru/company/selectel/blog/303190/)
* [Плейлист по DevOps](https://youtube.com/playlist?list=PLKdc04x0eX56fbDPVb3WUOtSEjZtYYq0n)
* [What Are Namespaces and cgroups, and How Do They Work?](https://www.nginx.com/blog/what-are-namespaces-cgroups-how-do-they-work/)
* [Недостающее введение в контейнеризацию](https://habr.com/ru/post/541288/)
* [Зачем нужен containerd и почему его отделили от Docker](https://habr.com/ru/company/flant/blog/325358/)
* [Understanding the Container Runtime Containerd in one article](https://www.sobyte.net/post/2021-09/containerd-usage/)
* [Как pod в Kubernetes получает IP-адрес](https://habr.com/ru/company/flant/blog/521406/)
* [Интеграция containerd с Kubernetes, заменяющая Docker, готова к production](https://habr.com/ru/company/flant/blog/414875/)
* [vDocker, containerd и CRI-O: Углубленное сравнение](https://itsecforu.ru/2022/03/16/%F0%9F%90%B3-docker-containerd-%D0%B8-cri-o-%D1%83%D0%B3%D0%BB%D1%83%D0%B1%D0%BB%D0%B5%D0%BD%D0%BD%D0%BE%D0%B5-%D1%81%D1%80%D0%B0%D0%B2%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5/)
* [Container Networking Interface (CNI) — сетевой интерфейс и стандарт для Linux-контейнеров](https://habr.com/ru/company/flant/blog/329830/)
* [Различия между Docker, containerd, CRI-O и runc](https://habr.com/ru/company/domclick/blog/566224/)
* [Иллюстрированное руководство по устройству сети в Kubernetes. Части 1 и 2](https://habr.com/ru/company/flant/blog/346304/)
* [Architecture of The CRI Plugin](https://github.com/containerd/cri/blob/v1.11.1/docs/architecture.md)